\documentclass[10pt,a4paper,conference]{IEEEtran}

\usepackage{color}
\usepackage{amsmath}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{threeparttable}
\usepackage[normalem]{ulem}
\usepackage[colorinlistoftodos]{todonotes}

\ifCLASSINFOpdf
\else
\fi

\hyphenation{}


\begin{document}

\title{Estimating Prediction Qualities without 
Ground Truth: A Revisit of the Reverse Testing Framework}

\author{\IEEEauthorblockN{Dheeraj Bhaskaruni}
\IEEEauthorblockA{Department of Computer Science\\
University of Wyoming\\
Laramie, Wyoming, USA\\
Email: vbhaskar@uwyo.edu}
\and
\IEEEauthorblockN{Fiona Patricia Moss}
\IEEEauthorblockA{Department of Computer Science\\
University of Wyoming\\
Laramie, Wyoming, USA\\
Email: fmoss1@uwyo.edu} 
\and 
\IEEEauthorblockN{Chao Lan}
\IEEEauthorblockA{Department of Computer Science\\
University of Wyoming \\ 
Laramie, Wyoming, USA\\
Email: clan@uwyo.edu} 
}

\maketitle

\begin{abstract}
To evaluate prediction qualities of machine learning models, 
it is typically assumed testing samples are labeled. 
However, testing labels are not always available in practice. 
A traditional solution is to approximate prediction qualities 
on testing samples by the qualities on labeled training samples. 
But this may be limited in that it completely ignores 
testing samples. 

In this paper, we present a new approach to estimate prediction 
qualities on unlabeled testing sample, based on the reverse 
testing framework \cite{fan2006reverse}. 
We evaluate the approach with various quality metrics in classification 
and anomaly detection tasks, and over numerous real-world data sets. 
Experimental results show the proposed approach gives a more 
accurate estimate of prediction qualities on testing sample than 
those on training samples. 
\end{abstract}


\IEEEpeerreviewmaketitle

\input{sec_introduction}

\input{sec_background}

\input{sec_methodology}

\input{sec_experiment}

\input{exp_classification}

\input{exp_anomalydetection}

\input{exp_disparity}

\subsection{Stationary Assumption} 

The proposed approaches are based on the 
stationary assumption. We verified this 
assumption by computing the z-scores between 
training and testing samples. Results averaged 
over 20 random trails on different data sets 
are 2e-4 for Spam Email, 2e-3 for Satellite, 
5e-4 for Cardio, 1e-2 for Ionosphere, 2e-5 for Malware, 
4e-5 for CID and 3e-4 for Arrhythmia. These results 
suggest there is no significant difference between 
training and testing distributions. 

This paper focuses on stationary setting. 
How to develop estimation approach 
for non-stationary setting is a future direction. 
Fan's work does not seem directly applicable, 
and one may consider combining it with techniques 
for correcting sample selection bias e.g. 
\cite{huang2007correcting}.


% \begin{table*}[t!]
% \renewcommand{\arraystretch}{1.5} 
% \caption{Z-Scores of all the data sets used in the experiments}
% \centering
% \begin{tabular}{|l|c|c|c|c|c|c|c|c|} \hline 
% \bf Data Set & Spam Email & Satellite & Cardio & Ionosphere & Malware & Network Intrusion & Arrhythmia \\ \hline 
% \bf Z-Score & 2e-4 & 2e-3 & 5e-4 & 1e-2 & 2e-5 & 4e-5 & 3e-4 \\  \hline 
% \end{tabular}
% \label{exp1:disparity}
% \end{table*}

\begin{table*}[t!]
\renewcommand{\arraystretch}{1.3} 
\caption{Classification Error and F1 Score Comparison on Six Data Sets}
\centering
\begin{threeparttable}
\setlength{\tabcolsep}{0.35em}
\begin{tabular}{llccccc|ccccc} %
\bf Data Set & \bf Classifier 
& $er(f; Q)$ & $er(f; S)$ & $er(f_{q}; S)$ &  $\Delta_{tr}$ & $\Delta_{retr}$ 
& $f1(f; Q)$ & $f1(f; S)$ & $f1(f_{q}; S)$ &  $\Delta_{tr}$ & $\Delta_{retr}$ \\ \hline 
S-Email & LogReg & .077 (3e-5) & .069 (2e-5) & .073 (2e-5) & .009 & .004  
& .901 (5e-5) & .911 (4e-5) & .905 (3e-5) & .010 & .004  \\ 
\ & LSVM & .189 (4e-5) & .054 (1e-5) & .187 (4e-5) & .135 & .003  
& .766 (5e-5)  & .928 (2e-5) & .774 (8e-5) & .162 & .008   \\ 
\ & kNN & .211 (4e-5) & .110 (3e-5) & .175 (5e-5) & .101 & .036 
& .729 (3e-5) & .858 (8e-5) & .771 (1e-4) & .128 & .042  \\ 
\ & DTree & .096 (4e-5) & .000 (0) & .091 (1e-4) & .096 & .005  
& .880 (6e-5) & .999 (0) & .091 (1e-4) & .120 & .006   \\ \hline 
Satellite & LogReg & .126 (2e-5) & .124 (2e-5) & .124 (3e-5) & .002 & .002  
& .770 (6e-5) & .775 (6e-5) & .774 (9e-5) & .004 & .003  \\  
\ & LSVM & .122 (2e-5) & .121 (2e-5) & .121 (2e-5) & .001 & .001 
& .772 (6e-5) & .775 (5e-5) & .774 (5e-5) & .002 & .002   \\ 
\ & kNN & .081 (3e-5) & .039 (7e-6) & .077 (2e-5) & .042 & .004   
& .871 (6e-5) & .938 (1e-5) & .877 (5e-5) & .066 & .006   \\ 
\ & DTree & .126 (4e-5) & 0 (0) & .129 (1e-4) & .126 & .003  
& .802 (1e-4) & 1 (0) & .799 (3e-4) & .198 & .003   \\ \hline 
Cardio & LogReg & .0120 (1e-5) & .014 (1e-5) & 0.021 (1e-5) & .005 & .001  
& .898 (4e-4) & .924 (3e-4) & .888 (3e-4) & .026 & .010 \\ 
\ & LSVM & .016  (1e-5) & .009 (1e-5) & .014 (1e-5) & .007 & .002
& .914 (3e-4) & .951 (2e-4) & .923 (4e-4) & .036 & .009  \\ 
\ & kNN & .018 (1e-5) & .011 (4e-6) & .019 (9e-6) & .007 & .001 
& .903 (4e-4) & .943 (9e-5) & .892 (2e-4) & .040 & .011 \\ 
\ & DTree & .019 (2e-5) & .004  (3e-6) & .017 (3e-5) & .015 & .003  
& .899 (5e-4) & .977 (6e-5) & .909 (1e-3) & .078 & .010 \\ \hline 
Iono. & LogReg & .143 (5e-4) & .076 (2e-4) & .139 (4e-4) & .067 & .004  
& .764 (1e-3) & .886 (5e-4) & .764 (1e-3) & .122 & .001 \\ 
\ & LSVM & .140 (4e-4) & .089 (1e-4) & .128 (3e-4) & .050 & .012 
& .776 (1e-3) & .866 (2e-4) & .792 (1e-3) & .091 & .017 \\ 
\ & kNN & .156 (1e-3) & .098 (1e-4) & .176 (1e-3) & .059 & .020 
& .733 (2e-3) & .846 (5e-4) & .692 (4e-3) & .114 & .041 \\ 
\ & DTree & .134 (1e-3) & .001 (8e-6) & .108 (8e-4) & .133 & .026  
& .813 (1e-3) & .998 (1e-5) & .845 (1e-3) & .186 & .032 \\ \hline 
Malware & LogReg & .387 (2e-4) & .387 (1e-4) & .438 (1e-4)& .001 & .051  
& .552 (3e-4) & .552 (1e-4) & .487 (4e-4) & .001 &  .065  \\
% \ & LSVM & .709 (8e-3) & .708 (8e-3) & .725 (1e-2) & .001 & .016 
% & .240 (8e-3) & .241 (8e-3) & .228 (1e-2) & .001 & .011  \\ 
\ & kNN & .214 (2e-5) & .119 (1e-5) & .181 (2e-5) & .095 & .033  
& .784 (2e-5) & .880 (1e-5) & .817(1e-5) & .096 & .033 \\  
\ & DTree & .039 (2e-5) & .022 (1e-5) & .041 (2e-5) & .016 & .002  
& .961 (2e-5) & .977 (1e-5) & .959 (2e-5) & .016 & .002 \\ \hline 
CID 
& LogReg & .426 (7e-3) & .423 (7e-3) & .446 (3e-3)& .003 & .020  
& .546 (9e-3) & .549 (9e-3) & .513 (4e-3) & .002 &  .033  \\
\ & LSVM & .316 (6e-5) & .313 (5e-5) & .314 (6e-5) & .003 & .002  
& .614 (6e-5) & .617 (5e-5) & .615 (6e-5) & .003 & .001 \\
\ & kNN & .023 (5e-6) & .013 (1e-6) & .022 (2e-6) & .010 & .001  
& .976 (5e-6) & .987 (1e-6) & .978 (2e-6) & .010 & .001 \\ 
\ & DTree & .025 (6e-6) & .016 (8e-6) & .027 (3e-5) & .009 & .002  
& .975 (6e-6) & .984 (8e-6) & .973 (3e-5) & .009 & .002 \\ \hline 
\end{tabular}
\end{threeparttable}
\label{tab1:accf1}
\end{table*}



\begin{table*}[t!]
\renewcommand{\arraystretch}{1.3} 
\caption{Anomaly Detection AUC Score Comparison on Six Data Sets}
\centering
\begin{threeparttable}
\begin{tabular}{llccccc} %
\bf Data Set & \bf Classifier & \bf $AUC(f; Q)$
& \bf $AUC(f; S)$ & \bf $\widetilde{AUC}(f; Q)$
&  $\Delta_{tr}$ & $\Delta_{retr}$ \\ \hline 
Spam Email & OC-SVM  & .7647 (7e-5) & .7672 (7e-5) 
& .7642 (7e-5) & .0025 & .0005  \\ 
\ & PCA & .8215 (1e-4) & .8372 (3e-4) & .8248 (4e-4) & .0157 & .0033  \\ 
\ & k-means & .7821 (5e-5) & .8666 (2e-5) & .7408 (5e-5) & .0845 & .0413  \\ 
\ & GMM & .7506 (7e-3) & .8282 (8e-5) & .7302 (4e-3) & .0776 & .0204  \\ \hline 
Satellite & OC-SVM & .3040 (3e-5) & .3028 (4e-5) & .3032 (3e-5) & .0012 & .0008  \\ 
\ & PCA & .8010 (5e-5) & .7984 (6e-5) & .7999 (5e-5) & .0026 & .0010  \\ 
\ & k-means & .8816 (3e-5) & .9807 (2e-6) & .8216 (3e-5) & .0991 & .0600 \\ 
\ & GMM & .8605 (7e-5) & .9544 (4e-5) & .8210 (8e-5) & .0939 & .0395 \\ \hline 
Arrhythmia & OC-SVM & .4625 (1e-3) & .4283 (1e-3) & .4512 (3e-3) & .0342 & .0114  \\ 
\ & PCA & .8371 (6e-4) & .8512 (5e-4) & .8341 (8e-4) & .0141 & .0030 \\ 
\ & k-means & .8250 (9e-4) & .9438 (2e-4) & .8162 (1e-3) & .1187 & .0088 \\ 
\ & GMM & .7975 (8e-4) & .9997 (0) & .7769 (2e-3) & .2022 & .0206 \\ \hline 
Iono. & OC-SVM & .1427 (4e-4) & .1235 (5e-4) & .1334 (5e-4) 
& .0193 & .0094  \\ 
\ & PCA & .9577 (2e-4) & .9782 (9e-5) & .9494 (2e-4) & .0205 & .0083 \\ 
\ & k-means & .9639 (3e-4) & .9962 (1e-5) & .9476 (4e-4) & .0323 & .0162 \\ 
\ & GMM & .9272 (2e-3) & .9999 (0) & .9394 (3e-4) & .0727 & .0123 \\ \hline 
Cardio & OC-SVM& .9853 (5e-5) & .9869 (3e-5) & .9857 (4e-5) & .0015 & .0003  \\ 
\ & PCA & .9499 (4e-5) & .9500 (4e-5) & .9461 (4e-5) & .0001 & .0038  \\ 
\ & k-means & .9408  (6e-5) & .9975 (1e-6) & .9030 (6e-5) & .0567 & .0378  \\ 
\ & GMM & .3227 (3e-3) & .8964 (2e-3) & .4159 (3e-3) & .5736 & .0932 \\ \hline 
CID & OC-SVM & .5013 (4e-5) & .4982 (4e-5) & .4982 (4e-5) & .0031 & .0031  \\ 
\ & PCA & .9990 (1e-6) & 1.0000 (0) & .9982 (2e-6) & .001 & .0008  \\ 
\ & k-means & .9909 (3e-6) & .9934 (3e-6) & .9922 (3e-6) & .0024 & .0013  \\ 
\ & GMM & .9980 (1e-6) & .9977 (1e-6) & .9905 (1e-6) & .0003 & .0072 \\ \hline 
\end{tabular}
\end{threeparttable}
\label{tab1:auc}
\end{table*}


\input{sec_conclusion}


\bibliographystyle{IEEEtran}
\bibliography{reference}

\end{document}




%\begin{figure}[h]
%\centering
%\includegraphics[width=.25\textwidth]{Backward_data_training.PNG} 
%\caption{Backward data evaluation Training}
%\end{figure}

