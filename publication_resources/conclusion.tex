\section{Conclusion}


In this section, we case study the proposed evaluation approaches 
on the Spam Email data set. In classification task, we show the 
proposed reversed training error is a more accurate estimate of 
testing error than traditional training error. In anomaly detection 
task, when training and testing samples are somewhat balanced, 
we show reversed detection AUC score is a more accurate estimate 
of testing AUC score than traditional training AUC score. 

We also examine the impact of using only confidently labeled 
testing instances to retrain model. This further improves 
the reversed training error and AUC score, but the improvements 
seem limited. 

Finally, in this section we only evaluated two metrics i.e. 
classification error and detection AUC score. We also focused 
on a single data set. 
In the next section, we extend these settings by additionally 
evaluating prediction f1 score and predictive disparity on 
more real-world data sets. 
